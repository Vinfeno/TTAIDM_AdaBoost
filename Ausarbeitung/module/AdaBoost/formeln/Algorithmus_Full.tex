\begin{algorithm}[H]
    \DontPrintSemicolon
    \LinesNotNumbered
    \KwData{Trainingsdatensatz \(D\), Anzahl der Iterationen \(T\).}
    \KwResult{Finale Klassifikationsfunktion: \(H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)\).}
    \BlankLine
    \tcp{Initialisiere Gewichte}
    \(w^{(1)}_i=\frac{1}{m}\)\;
    \For{\(t = 1\) \KwTo \(T\)}{
    \tcp{Trainiere schwache Lerner}
    \((h_{j})_{j\in\mathcal{I}} \leftarrow \mathcal{L}(D, w^{(t)}_i)\) \;
    \tcp{Berechne Fehler}
    \For{$j=1$ \KwTo $n$ }{
        \(\varepsilon_j = \sum_{i=1}^{m} w^{(t)}_i\cdot I(y_i \neq h_j(x_i))\)\;
    }
    Wähle Lerner $h_j$ mit minimalem Fehler $\varepsilon_j$ als \(h_t\) mit Fehler $\varepsilon_t$\;
    \tcp{Berechne den Lernerkoeffizienten}
    \(\alpha_t = \frac{1}{2} \ln \left( \frac{1 - \varepsilon_t}{\varepsilon_t} \right)\)\;
    \tcp{Aktualisiere die Gewichte für die nächsten Iterationen}
    \eIf{\(y_i = h_t(x_i)\)}{
        \(w^{(t+1)}_i \leftarrow w^{(t)}_i \cdot e^{-\alpha_t}\)\;
    }{
        \(w^{(t+1)}_i \leftarrow w^{(t)}_i \cdot e^{\alpha_t}\)\;
    }
    \tcp{Normalisiere Gewichte}
    \(Z_t \leftarrow \sum_{j=1}^mw^{(t+1)}_i\)\;
    \For{\(i = 1\) \KwTo \(m\)}{
        \(w^{(t+1)}_i \leftarrow \frac{w^{(t+1)}_i}{Z_t}\)\;
    }
    }
    \KwOut{\(H(x)=\text{sign}\left(\sum_{t=1}^T\alpha_th_t(x)\right)\)}
    \tcp{Ende des Algorithmus}
\end{algorithm}



% \input{../Ausarbeitung/module/AdaBoost/formeln/Algo1.tex}
% \input{../Ausarbeitung/module/AdaBoost/formeln/Algo2.tex}