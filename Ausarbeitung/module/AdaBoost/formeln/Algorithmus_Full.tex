\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwData{Trainingsdatensatz \(D\), wobei \(x_i \in \mathcal{X}\) und \(y_i \in \{-1, 1\}\); Anzahl der Iterationen \(T\).}
    \KwResult{Finale Klassifikationsfunktion: \(H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)\).}
    \BlankLine
    \tcp{Initialisiere Gewichte}
    \(\mathcal{D}_1(i)=\frac{1}{n}\)\;
    \For{\(t = 1\) \KwTo \(T\)}{
        \tcp{Trainiere einen schwachen Lerner unter Berücksichtigung der Gewichtung \(\mathcal{D}_t(i)\)}
        \(h_t \leftarrow \mathcal{L}(D, \mathcal{D}_t)\) \;
        \tcp{Berechne den gewichteten Fehler}
        \(\epsilon_t = \sum_{i=1}^{n} \mathcal{D}_t(i) I(y_i \neq h_t(x_i))\)\;
        Wähle den Lerner mit dem geringsten Fehler als \(h_t\)\;
        \If{\(\epsilon_t > 0.5\)}{\textbf{break}}\;
        \tcp{Berechne den Lernerkoeffizienten}
        \(\alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right)\)\;
        \tcp{Aktualisiere die Gewichte}
        \For{\(i = 1\) \KwTo \(n\)}{
            \eIf{\(y_i = h_t(x_i)\)}{
                \(\mathcal{D}_{t+1}(i) \leftarrow \mathcal{D}_{t}(i) \times e^{-\alpha_t}\)\;
            }{
                \(\mathcal{D}_{t+1}(i) \leftarrow \mathcal{D}_{t}(i) \times e^{\alpha_t}\)\;
            }
        }\;
        \tcp{Normalisiere Gewichte}
        \(Z_t \leftarrow \sum_{j=1}^n\mathcal{D}_{t+1}(j)\)\;
        \For{\(i = 1\) \KwTo \(n\)}{
            \(\mathcal{D}_{t+1}(i) \leftarrow \frac{\mathcal{D}_{t+1}(i)}{Z_t}\)\;
        }
    }
    \KwOut{\(H(x)=\text{sign}\left(\sum_{t=1}^T\alpha_th_t(x)\right)\)}
\end{algorithm}



% \input{../Ausarbeitung/module/AdaBoost/formeln/Algo1.tex}
% \input{../Ausarbeitung/module/AdaBoost/formeln/Algo2.tex}