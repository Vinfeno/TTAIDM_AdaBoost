AdaBoost, kurz für \glqq Adaptive Boosting\grqq, wurde in den 1990ern von Yoav Freund und Robert Schapire für die binäre
Klassifikation konzipiert und hat das Feld des maschinellen Lernens maßgeblich geprägt. Während Boosting-Methoden
allgemein iterativ arbeiten und Fehler der vorherigen Modelle korrigieren, zeichnet sich AdaBoost durch seine spezielle
Methode zur Gewichtungsanpassung der Datenpunkte aus. In jeder Trainingsiteration erhöht AdaBoost gezielt die Gewichtung
der falsch klassifizierten Datenpunkte, wodurch er kontinuierlich seine Vorhersagegenauigkeit verbessert. Dieser
einzigartige und adaptive Ansatz zur Fehlerkorrektur unterscheidet AdaBoost von anderen Boosting-Methoden und hat
nicht nur die Effizienz von Boosting-Methoden für binäre Klassifikationsprobleme unter Beweis gestellt, sondern auch zu
zahlreichen Weiterentwicklungen in diesem Bereich angeregt. \cite{WuKumar2009}