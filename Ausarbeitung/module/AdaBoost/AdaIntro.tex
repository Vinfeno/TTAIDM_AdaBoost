AdaBoost, kurz für \glqq Adaptive Boosting\grqq, wurde in den 1990ern von Yoav Freund und Robert Schapire für die binäre
Klassifikation konzipiert und hat das Feld des maschinellen Lernens beeinflusst. Während Boosting-Methoden
allgemein iterativ arbeiten und Fehler der vorherigen Modelle korrigieren, zeichnet sich AdaBoost durch seine spezielle
Methode zur Gewichtungsanpassung der Datenpunkte aus. In jeder Trainingsiteration erhöht AdaBoost gezielt die Gewichtung
der falsch klassifizierten Datenpunkte exponentiell abhängig davon, wie gut der Lerner die Daten insgesamt vorhersagt.
Dieser adaptive Ansatz zur Fehlerkorrektur unterscheidet AdaBoost von anderen Boosting-Methoden und hat die Effizienz von
Boosting-Methoden für binäre Klassifikationsprobleme gezeigt und viele
Weiterentwicklungen in diesem Bereich angeregt. \cite{WuKumar2009}