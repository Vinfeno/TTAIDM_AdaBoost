AdaBoost, ursprünglich für binäre Klassifikation entwickelt,
wurde durch verschiedene Erweiterungen für diverse Problemstellungen adaptiert.

\begin{itemize}
    \item Variationen wie \glqq AdaBoost.M1\grqq~ und \glqq SAMME\grqq~ erweitern den Algorithmus für Multiklassen-Probleme. \cite{hastie2009multi}
    \item Kosten-sensitives AdaBoost passt Gewichtungen basierend auf bestimmten Fehlerarten an. \cite{masnadi2010cost}
    \item Neben Entscheidungsstümpfen kann AdaBoost mit SVMs, Neuronalen Netzen oder jedem anderen Klassifikator kombiniert werden. \cite{zhang2016stock}
    \item Robuste AdaBoost-Varianten minimieren die Auswirkung von Ausreißern. \cite{viola2001fast}
    \item Online AdaBoost aktualisiert Modelle mit sequenziellen Daten ohne Neutrainierung. \cite{hu2013online}
    \item Einige Varianten integrieren Feature-Auswahl direkt, um Interpretierbarkeit und Trainingseffizienz zu steigern. \cite{wu2003learning}
\end{itemize}