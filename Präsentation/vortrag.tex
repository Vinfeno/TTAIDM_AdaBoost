%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Presentation template for Scientific Computing seminar
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[hyperref={bookmarks=false},11pt,dvipsnames]{beamer}
\usepackage{tabularx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\setbeamercolor{url}{fg=red}
\usepackage{mathtools}
\definecolor{buwgreen}{rgb}{0.537,0.729,0.090}
\definecolor{darkred}{rgb}{0.8, 0, 0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Course metadata
\newcommand{\coursename}{Bachelor-Seminar \glqq{}Top 10 Algorithms in Data Mining\grqq{}}
\newcommand{\coursenamefootline}{Bachelor-Seminar \glqq{}Top 10 Algorithms in Data Mining\grqq{}}

% In seminars, the presenter of a talk is typically different from the lecturer responsible for the course.
% The following commands allow to have this distinction. The lecturer's name is printed in the banner on the 
% titlepage (if activated) and the presenter's name is passed into the \author{} command.
\newcommand{\presentername}{Marius Graf}
\newcommand{\presenternameshort}{M. Graf}
\newcommand{\lecturername}{Dr.~Marcel Schweitzer}

\author[\presenternameshort]{\presentername}
\institute{Bergische Universität Wuppertal}
\def\englishlanguage{0}             % set to 1 to switch from German to English

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Switches for title page design
\def\printbanner{1}                 % set to 1 to print title page banner
\def\printauthor{1}                 % set to 1 if author name should also be printed outside of title banner
\def\printdate{1}                   % set to 1 in order to print date
\def\printinstitute{0} 		          % set to 1 in order to print institute
\def\printcoursename{1}		          % set to 1 to print course name above title and in footline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Other layout switches
\def\coveredtransparent{1}          % set to 1 to make covered items completely invisible
\def\printnavigationsymbols{0}      % set to 1 to activate navigation symbols
\def\printtocatbeginofsection{1}    % print outline slide (with highlighted current section) at beginning of each section
\def\printtocatbeginofsubsection{0} % print outline slide (with highlighted current subsection) at beginning of each subsection
\def\printlion{1}                   % set to 0 to suppress small "Uni-Loewe" icon in top right corner
\def\printpagenumbers{1}            % set to 0 to suppress page numbers in foot line
\def\longtitle{0}                   % Sometimes, very long titles can break the title page layout. In that case, setting this to 1 might improve things

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{slidestyle} 
% \DeclareMathOperator*{\sign}{sign}

\title{AdaBoost}
\date{06.12.2023}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title slide
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% outline slide (without frame number in foot line)
\let\rememberpagenumberswitch\printpagenumbers
\def\printpagenumbers{0}

\begin{frame}[t,noframenumbering]{\ifthenelse{\englishlanguage=1}{Outline}{Inhalt}}
	\tableofcontents
\end{frame}
\let\printpagenumbers\rememberpagenumberswitch
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Einleitung}

\begin{frame}[t]{Was ist Data Mining?}
	\includegraphics[width=0.5\textwidth]{figures/datamining.png}
	\begin{itemize}
		\item <1-> \textbf{Analysiert} große Datenmengen, um Muster und Zusammenhänge zu erkennen.
		\item <2-> Nutzt dabei Methoden aus der \textbf{Statistik}, dem  \textbf{Machine Learning} und Datenbanktechnologien.
		\item <3-> Spielt zentrale Rolle in der Forschung und Industrie,
		      um Erkenntnisse zu gewinnen und
		      Entscheidungen zu unterstützen.
	\end{itemize}
\end{frame}
\begin{frame}[t]{Was sind Ensemble-Methoden?}
	\begin{itemize}
		\item <1-> \textbf{Ensemble-Verfahren:} \textbf{Kombinieren} mehrere Modelle für präzisere Vorhersagen
		\item <2-> \textbf{Fehlerminimierung:} Reduzieren von \textbf{systematischen Fehlern} in Modellprognosen
		\item <3-> \textbf{Arten von Ensemble-Methoden:}\\[5pt]
		      \begin{itemize}
			      \item Bagging
			      \item Stacking
			      \item Boosting
		      \end{itemize}
		\item <4-> \textbf{AdaBoost gehört zu den Boosting-Verfahren}
	\end{itemize}
\end{frame}

\section{Grundlagen des Boosting}
\begin{frame}[t]{Die Grundidee}
	\begin{itemize}
		\item <1-> Boosting kombiniert \textbf{schwache Lerner} zu einem \textbf{starken Gesamtmodell}
		\item <2-> \textbf{Schwacher Lerner:} Modell, das nur \textbf{geringfügig besser} ist als \textbf{zufälliges Raten}
		\item <3-> \textbf{Passe Gewichtung} der Trainingsdaten iterativ \textbf{an}, damit neue Modelle die \textbf{Fehler der Vorgänger korrigieren}
		\item <4-> $\leadsto$ verrignerter \textbf{Bias, bessere Vorhersagegenauigkeit} für schwer klassifizierbare Beispiele
	\end{itemize}
\end{frame}

\begin{frame}[t]{Veranschaulichung}
	\centering
	\includegraphics[width=0.8\textwidth]{../Ausarbeitung/figures/Boosting_Graph.png}
\end{frame}
\begin{frame}[t]{Beispiel}{Vorhersage von Hauspreisen}
	\begin{itemize}
		\item <1-> Wir möchten ein \textbf{Modell} entwickeln, das den \textbf{Preis von
			      Häusern} basierend auf \textbf{verschiedenen Merkmalen} wie Größe, Lage, Anzahl der
		      Zimmer und Baujahr \textbf{vorhersagt}.
	\end{itemize}
	\begin{table}
		\centering
		\resizebox{\textwidth}{!}{%
			\input{../Ausarbeitung/module/boosting/BoostingTabelle1.tex}
		}
	\end{table}


\end{frame}
\subsection*{Beispiel}
\begin{frame}[t]{Beispiel}{Vorhersage von Hauspreisen}
	\begin{itemize}
		\item <1-> \textbf{Einfaches Modell} (schwacher Lerner): Preisvorhersage nur anhand von \textbf{Größe}
		\item <2-> Tatsächlich spielen auch \textbf{andere Faktoren} (z.B. Lage) eine Rolle
		\item <3-> $\leadsto$ \textbf{Bias} des schwachen Lerners:\\[5pt]
		      \begin{itemize}
			      \item Preis von Häusern in guter Lage wird \textbf{unterschätzt}
			      \item Preis von Häusern in schlechter Lage wird \textbf{überschätzt}
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Beispiel}{Vorhersage von Hauspreisen}
	\begin{itemize}
		\item \textbf{Boosting:} Passe iterativ Gewicht der Datenpunkte so an, dass nächstes Modell verstärkt die schlecht vorhergesagten Fälle beachtet
	\end{itemize}
	\emph{P = Vorhersage, W = Gewichtung}
	\begin{table}
		\centering
		\resizebox{0.95\textwidth}{!}{%
			\input{../Ausarbeitung/module/boosting/BoostingTabelle2.tex}
		}
		\resizebox{0.95\textwidth}{!}{%
			\input{../Ausarbeitung/module/boosting/BoostingTabelle3.tex}
		}
	\end{table}
\end{frame}

\section{Der AdaBoost Algorithmus}
\begin{frame}[t]{Einführung}
	\begin{itemize}
		\item <1-> \glqq Adaptive Boosting\grqq
		\item <2-> entwickelt in den 1990ern von Freund und Schapire, einflussreiches Verfahren für \textbf{binäre Klassifikation}
		\item <3-> Boosting-Methode, \textbf{Gewichtung} falsch klassifizierter Datenpunkte \textbf{exponentiell} erhöht
		\item <4-> spezielle \textbf{adaptive} Fehlerkorrektur
	\end{itemize}
\end{frame}

\begin{frame}{Vereinfachte Sicht auf den Algorithmus}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\LinesNotNumbered
		\SetKwInput{KwData}{Daten}
		\SetKwInput{KwResult}{Ergebnis}
		\BlankLine
		\KwIn{Datensatz, Lernalgorithmus}
		Initialisiere Gewichte des Datensatzes\;
		\For{\(t = 1\) \KwTo \(T\)}{
			Trainiere schwache Lerner mit gewichtetem Datensatz\;
			Bestimme Fehler der Lerner\;
			Wähle schwachen Lerner mit geringstem Fehler\;
			Berechne Lernkoeffizienten\;
			Gewichte Datenpunkte neu\;
		}
		\KwOut{\text{Starker Lerner (Ensemble)}}
	\end{algorithm}
\end{frame}

\begin{frame}[t]{Notation}
	\begin{itemize}
		\item <1-> $\mathcal{X}:$ Menge der \textbf{Features}, $|\mathcal{X}|=n$
		\item <2-> $\mathcal{Y}:$ Menge der \textbf{Labels} $(\mathcal{Y}=\{-1,+1\}$ bei binärer Klassifikation)
		\item <3-> $D:$ \textbf{Trainingdatensatz} der Form $D=\{(\boldsymbol{x}_i,y_i)\},~i=1,...,m$
		\item <4-> Modell wird auf $D$ durch \textbf{Lernalgorithmus} $\mathcal{L}$ (meistens \emph{Decision Stump}) trainiert und gilt
		\item <5-> eine \textbf{Hypothese} $h:X\rightarrow\mathcal{Y},~h(\boldsymbol{x})=y$ zurück
		\item <6-> Anzahl der \textbf{Trainingsiterationen} $T$
		\item <7-> bei jeder Iteration wird $D$ um \textbf{Gewichte} $$w^{(t)}_i$$ mit $i=1,\dots,m$ und $t=1,\dots,T$ erweitert
	\end{itemize}
\end{frame}

\begin{frame}[t]{Initialisierung der Gewichte}
	\begin{itemize}
		\item <1-> Zu beginn sind die Gewichte gleich verteilt \input{../Ausarbeitung/module/AdaBoost/formeln/Init1.tex}
		\item <2-> Die Summe der Gewichte ist stets 1 \input{../Ausarbeitung/module/AdaBoost/formeln/Init2.tex}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Training der schwachen Lerner}
	\begin{itemize}
		\item <1-> \textbf{Trainiere} pro Iteration mehrere \textbf{schwache Lerner} (für jedes Feature zwei, je mit umgekehrter Polarität)
		      $$
			      h = \mathcal{L}(D, w^{(t)})
		      $$
		      mit $w^{(t)}$ als Gewichte der $t$-ten Iteration
		\item <2-> Ziel: gewichteten \textbf{Fehler minimieren}
		      $$
			      \varepsilon_j = \sum_{i=1}^n w_i^{(t)}\cdot I\left(y_i \neq h_j\left(\boldsymbol{x}_i\right)\right),~j=1,\dots,|h|
		      $$
		      Wähle Lerner $h_j\in h$ mit \textbf{geringstem Fehler} $\varepsilon_j$ (meiste korrekt klassifizierte Datenpunkte)
		\item <3-> Dabei bezeichntet $I$ die \textbf{Indikatorfunktion}
		      $$
			      I(A)=\left\{\begin{array}{l l}
				      1, & \text{wenn } A \\
				      0, & \text{sonst.}
			      \end{array}\right.
		      $$
	\end{itemize}
\end{frame}

\begin{frame}[t]{Berechnung des Lernkoeffizienten}
	\begin{itemize}
		\item <1-> Exponentieller Verlust \begin{align*}
			      L(h_t) = \sum_{i=1}^{m} w_i^{(t)}e^{-y_ih_t(x_i)}
		      \end{align*}
		\item <2-> Herleitung: führe Lernkoeffizienten $\alpha_t$ ein \begin{align*}
			      L(h_t)=\sum_{i=1}^{m}w_ie^{-\alpha_ty_ih_t(x_i)}
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Berechnung des Lernkoeffizienten}
	\begin{itemize}
		\item <1-> [] \begin{align*}
			      y_i = h_t(x_i)    & \implies y_ih_t(x_i)  = 1                                     \\
			                        & \leadsto \text{ Beitrag zum Verlust: } w_i^{(t)}e^{-\alpha_t} \\
			      y_i \neq h_t(x_i) & \implies y_ih_t(x_i)  = -1                                    \\
			                        & \leadsto \text{ Beitrag zum Verlust: } w_i^{(t)}e^{\alpha_t}
		      \end{align*}
		\item <2-> Minimieren von $L(h_t)$:\begin{align*}
			      L(h_t) = \sum_{y_i=h_t(x_i)}w_i^{(t)}e^{-\alpha_t} + \sum_{y_i\neq h_t(x_i)} w_i^{(t)}e^{\alpha_t} \\
		      \end{align*}
		\item<3->[] \begin{align*}
			      \frac{dL(h_t)}{d\alpha_t} = -e^{-\alpha_t}\sum_{y_i=h_t(x_i)}w_i^{(t)} + e^{\alpha_t}\sum_{y_i\neq h_t(x_i)}w_i^{(t)} = 0
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Berechnung des Lernkoeffizienten}
	\begin{itemize}
		\item <1-> [] \begin{align*}
			       & \Leftrightarrow e^{2\alpha_t} = \frac{\sum_{y_i=h_t(x_i)}w_i}{\sum_{y_i\neq h_t(x_i)}w_i}                       \\
			       & \Leftrightarrow \alpha_t = \frac{1}{2}\ln\left(\frac{\sum_{y_i=h_t(x_i)}w_i}{\sum_{y_i\neq h_t(x_i)}w_i}\right)
		      \end{align*}
		\item<2-> Da $\sum_{y_i=h_t(x_i)}w_i=1-\varepsilon_t$ und $\sum_{y_i\neq h_t(x_i)}w_i=\varepsilon_t$\begin{align*}
			      \implies \alpha_t =\frac{1}{2}\ln\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Berechnung des Lernkoeffizienten}
	\begin{align*}
		\alpha_t =\frac{1}{2}\ln\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)
	\end{align*}
	\centering\includegraphics[width=0.6\textwidth]{../Ausarbeitung/figures/alpha_graph.png}
\end{frame}

\begin{frame}[t]{Aktualisierung der Gewichte}
	\begin{itemize}
		\item <1-> Neue Gewichte der Daten für den nächsten Durchlauf berechnen
		      \begin{align*}
			      \begin{array}{l l l}
				      w^{(t+1)}_i & = w^{(t)}_i \cdot e^{-\alpha_t} & \text{für korrekt klassifizierte Datenpunke} \\
				      w^{(t+1)}_i & = w^{(t)}_i \cdot e^{\alpha_t}  & \text{für falsch klassifizierte Datenpunke}
			      \end{array}
		      \end{align*}
		\item <2-> Die neuen Gewichte müssen anschließend normalisiert werden, damit ihre Summe wieder 1 ist:
		      \begin{align*}
			      Z_t         & =\sum_{j=1}^m w^{(t+1)}_i\qquad(\text{Normalisierungsfaktor}) \\
			      w^{(t+1)}_i & = \frac{w^{(t+1)}_i}{Z_t}
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}[t]{Das Ergebnis des Algorithmus}
	Der Algorithmus gibt ein Gesamtmodell zurück, welches die Klassifizierung des Datenpunktes durch die gewichtete
	Summe aller schwachen Lerner darstellt:
	\begin{align*}
		H    & :      X \rightarrow \{-1, +1\}                 \\
		H(x) & =  \sign\left(\sum_{t=1}^T\alpha_th_t(x)\right)
	\end{align*}
\end{frame}

\begin{frame}[allowframebreaks]{Der vollständige Algorithmus}
	\begin{scriptsize}
		\input{../Ausarbeitung/module/AdaBoost/formeln/Algo1.tex}
		\input{../Ausarbeitung/module/AdaBoost/formeln/Algo2.tex}
	\end{scriptsize}
\end{frame}

\begin{frame}{Beispiel}{Das XOR-Problem (eine Variation)}
	\centering
	\includegraphics[width=0.5\textwidth]{../Ausarbeitung/figures/XOR-Problem.png}
\end{frame}

\section{Praktische Anwendung}
\begin{frame}{Praktische Anwendung und Beispiele}{\textbf{Bilderkennung und Computervision:} Gesichtserkennung}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{../Ausarbeitung/figures/CV_Example.png}
	\end{figure}
\end{frame}
\begin{frame}{Praktische Anwendung und Beispiele}{\textbf{Textklassifikation und Natural Language Processing}: Erkennung von Spam-Mail}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{../Ausarbeitung/figures/spam.png}
	\end{figure}
\end{frame}
\begin{frame}{Praktische Anwendung und Beispiele}{\textbf{Medizinische Diagnostik:} Risiko/Erkennung von Krankheiten baserend auf Patientendaten}
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{../Ausarbeitung/figures/ada-whips.png}
	\end{figure}
\end{frame}
\begin{frame}{Praktische Anwendung und Beispiele}{\textbf{Finanzwesen:} Vorhersage von Aktienkursbewegungen}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{../Ausarbeitung/figures/stock.png}
	\end{figure}
\end{frame}

\section{Vor- und Nachteile}
\begin{frame}{Vor- und Nachteile von AdaBoost}
	\begin{itemize}
		\item <1-> []\textbf{Vorteile:}\\[5pt]
		      \begin{itemize}
			      \item<2-> [\textbf{\textcolor{buwgreen}{+}}] Benutzerfreundlich
			      \item<3-> [\textbf{\textcolor{buwgreen}{+}}] Flexibel
			      \item<4-> [\textbf{\textcolor{buwgreen}{+}}] Identifiziert automatisch wichtige Features
			      \item<5-> [\textbf{\textcolor{buwgreen}{+}}] Neigt weniger zum Overfitting
		      \end{itemize}
		\item <6-> [] \textbf{Nachteile:}\\[5pt]
		      \begin{itemize}
			      \item<7-> [\textbf{\textcolor{darkred}{-}}] Anfällig für \textbf{verrauschte Daten und Ausreißer}
			      \item<8-> [\textbf{\textcolor{darkred}{-}}] Training auf großen Datensätzen kann \textbf{zeitintensiv} sein
			      \item<9-> [\textbf{\textcolor{darkred}{-}}] Hauptsächlich für \textbf{binäre Klassifikation} ausgelegt
		      \end{itemize}
	\end{itemize}
\end{frame}

\section{Erweiterungen und Variationen}
\begin{frame}{Erweiterungen und Variationen von AdaBoost}
	\begin{itemize}
		\item <1-> Ursprünglich für binäre Klassifikation entwickelt,
		      durch verschiedene Erweiterungen für diverse Problemstellungen adaptiert
		\item <2-> \glqq AdaBoost.M1\grqq~ und \glqq SAMME\grqq~ für \textbf{Multiklassen-Probleme}
		\item <3-> \textbf{Kosten-sensitives} AdaBoost
		\item <4-> Neben Decision Stumps kann AdaBoost mit \textbf{SVMs, Neuronalen Netzen und anderen Classifiern} kombiniert werden

	\end{itemize}
\end{frame}

\begin{frame}{Erweiterungen und Variationen von AdaBoost}
	\begin{itemize}
		\item <1-> \textbf{Robuste} Varianten minimieren die Auswirkung von Ausreißern.
		\item <2-> \textbf{Online} AdaBoost aktualisiert Modelle ohne Neutrainierung.
		\item <3-> \textbf{Direkte Feature Auswahl:} Wählt während des Trainings aus, welche Features wichtig sind
		      und betrachtet nur diese $\leadsto$ \textbf{schnelleres Training}
		\item <4-> Variationen, welche die \textbf{Interpretierbarkeit und Erklärbarkeit} verbessert
	\end{itemize}
\end{frame}
\section{Literatur und Zusatzmaterial}
\begin{frame}[allowframebreaks]{Literatur}
	\nocite{*}
	\bibliographystyle{alpha}
	\bibliography{../Ausarbeitung/seminar_top10.bib}
\end{frame}
\begin{frame}{Zusatzmaterial}
	\centering
	\includegraphics[width=0.45\textwidth]{../Code/img/XOR_Code.png}
	\includegraphics[width=0.45\textwidth]{../Code/img/face_recognition.png}\\
	Umsetzungen und Beispiele von AdaBoost + diese Präsentation mit Ausarbeitung
	in \LaTeX~auf \textcolor{buwgreen}{\emph{\underline{\href{https://github.com/Vinfeno/TTAIDM_AdaBoost}{GitHub}}}}.
\end{frame}

\begin{frame}{Danke}
	\resizebox{\linewidth}{!}{Vielen Dank für die Aufmerksamkeit!}
\end{frame}
\end{document}



